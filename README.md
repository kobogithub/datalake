# ğŸ—ƒï¸ Data Lake & Synthetic Data Pipeline

Pipeline completo para generar datos sintÃ©ticos, convertir a formato Parquet optimizado y realizar anÃ¡lisis con DuckDB.

## ğŸš€ CaracterÃ­sticas

- **GeneraciÃ³n de datos sintÃ©ticos** con mÃºltiples mÃ©todos
- **ConversiÃ³n CSV â†’ Parquet** con particionamiento inteligente
- **AnÃ¡lisis de datos** con DuckDB optimizado
- **Metadata avanzada** y consultas de rendimiento

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ data-synthetic-json/         # GeneraciÃ³n con OpenAI/Faker
â”œâ”€â”€ data-synthetic-producer/     # Generador masivo de datos
â”œâ”€â”€ parquet/                     # Conversores CSV â†’ Parquet
â”œâ”€â”€ duckdb/                      # AnÃ¡lisis y consultas
â””â”€â”€ requirements.txt             # Dependencias
```

## âš¡ Quick Start

### 1. InstalaciÃ³n
```bash
git clone <repo>
cd datalake
pip install -r requirements.txt
```

### 2. Generar Datos SintÃ©ticos
```bash
# OpciÃ³n A: Con Faker (recomendado)
cd data-synthetic-json
python data-synthetic.py

# OpciÃ³n B: Generador masivo
cd data-synthetic-producer  
python data-synthetic-producer.py
```

### 3. Convertir a Parquet
```bash
cd parquet
# ConversiÃ³n bÃ¡sica
python data-parquet.py

# Con compresiÃ³n especÃ­fica (gzip, snappy, brotli)
python data-parquet-comprimido.py
```

### 4. AnÃ¡lisis con DuckDB
```bash
cd duckdb
# AnÃ¡lisis interactivo
python duckdb.py

# O directamente en DuckDB CLI
duckdb
D SELECT * FROM read_parquet('../parquet_data/**/*.parquet') LIMIT 5;
```

## ğŸ“Š Datasets Generados

| Dataset | Registros | Columnas | Casos de Uso |
|---------|-----------|----------|--------------|
| **Empleados** | 200K+ | 19 | RRHH, anÃ¡lisis salarial, performance |
| **Ventas** | 500K+ | 16 | E-commerce, tendencias, segmentaciÃ³n |
| **Marketing** | 300K+ | 20 | ROI, canales, optimizaciÃ³n campaÃ±as |

## ğŸ› ï¸ Componentes

### ğŸ² GeneraciÃ³n de Datos
- **`data-synthetic.py`**: Faker + distribuciones estadÃ­sticas
- **`data-synthetic-producer.py`**: GeneraciÃ³n masiva optimizada
- Datos realistas con correlaciones lÃ³gicas

### ğŸ“¦ ConversiÃ³n Parquet
- **`data-parquet.py`**: Conversor bÃ¡sico con particionamiento
- **`data-parquet-comprimido.py`**: MÃºltiples opciones de compresiÃ³n
- OptimizaciÃ³n automÃ¡tica de tipos de datos

### ğŸ¦† AnÃ¡lisis DuckDB
- **`duckdb.py`**: Analizador interactivo con consultas predefinidas
- **`queries.sql`**: +50 consultas de ejemplo
- **`metadata.sql`**: InspecciÃ³n de metadata de archivos Parquet

## ğŸ“ˆ Ejemplos de Uso

### Generar 1M de registros
```python
from data_synthetic_producer import DatasetGeneratorFaker
gen = DatasetGeneratorFaker()
gen.generar_todos_los_datasets()  # CSV files
```

### Convertir a Parquet optimizado
```python
from data_parquet_comprimido import ParquetCompressionConverter
converter = ParquetCompressionConverter(compression='gzip')
converter.convertir_todos_con_compression()
```

### AnÃ¡lisis ultrarrÃ¡pido
```sql
-- 1M+ registros en <100ms
SELECT 
    departamento,
    COUNT(*) as empleados,
    AVG(salario_anual) as salario_promedio
FROM read_parquet('parquet_data/empleados_gzip/**/*.parquet')
GROUP BY departamento;
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno
```bash
export OPENAI_API_KEY="sk-..."  # Para generaciÃ³n con IA
```

### OptimizaciÃ³n DuckDB
```sql
SET threads TO -1;              -- Usar todos los cores
SET memory_limit = '4GB';       -- Aumentar memoria
```

### Opciones de CompresiÃ³n
| CompresiÃ³n | Velocidad | TamaÃ±o | Uso |
|------------|-----------|---------|-----|
| `snappy` | â­â­â­â­â­ | â­â­â­ | AnÃ¡lisis interactivo |
| `gzip` | â­â­â­ | â­â­â­â­â­ | Archivado |
| `brotli` | â­â­ | â­â­â­â­â­ | MÃ¡xima compresiÃ³n |

## ğŸ“Š Benchmarks

### Rendimiento TÃ­pico
- **GeneraciÃ³n**: 100K registros/segundo
- **ConversiÃ³n**: 60-80% reducciÃ³n de tamaÃ±o
- **Consultas**: 1M+ registros en <100ms

### ComparaciÃ³n de Formatos
| Formato | TamaÃ±o | Velocidad Lectura | Compatibilidad |
|---------|--------|------------------|----------------|
| CSV | 100MB | 1x | Universal |
| Parquet | 30MB | 10x | Moderna |

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“ Dependencias

```txt
pandas>=2.0.0
pyarrow>=10.0.0
duckdb>=0.9.0
faker>=20.0.0
numpy>=1.24.0
openai>=1.0.0
python-dotenv>=1.0.0
```

## ğŸ”— Enlaces Ãštiles

- [DuckDB Documentation](https://duckdb.org/docs/)
- [Parquet Format](https://parquet.apache.org/)
- [Faker Documentation](https://faker.readthedocs.io/)

## ğŸ“„ Licencia

MIT License - ver [LICENSE](LICENSE) para detalles.

---

â­ Si este proyecto te ayuda, Â¡dale una estrella!